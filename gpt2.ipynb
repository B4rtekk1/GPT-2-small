{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde5d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPT2Config:\n",
    "    block_size: int = 256\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 8\n",
    "    n_head: int = 8\n",
    "    d_model: int = 784\n",
    "    dropout: float = 0.1\n",
    "    d_ff: int | None = None\n",
    "    activation_function: str = 'gelu'\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.d_ff is None:\n",
    "            self.d_ff = 4 * self.d_model\n",
    "        if not isinstance(self.block_size, int) or self.block_size <= 0:\n",
    "            raise ValueError(\"block_size must be an integer > 0\")\n",
    "        if not isinstance(self.vocab_size, int) or self.vocab_size <= 0:\n",
    "            raise ValueError(\"vocab_size must be an integer > 0\")\n",
    "        if not isinstance(self.n_layer, int) or self.n_layer <= 0:\n",
    "            raise ValueError(\"n_layer must be an integer > 0\")\n",
    "        if not isinstance(self.n_head, int) or self.n_head <= 0:\n",
    "            raise ValueError(\"n_head must be an integer > 0\")\n",
    "        if not isinstance(self.d_model, int) or self.d_model <= 0:\n",
    "            raise ValueError(\"d_model must be an integer > 0\")\n",
    "        if self.d_model % self.n_head != 0:\n",
    "            raise ValueError(\"d_model must be divisible by n_head\")\n",
    "        if not isinstance(self.d_ff, int) or self.d_ff <= 0:\n",
    "            raise ValueError(\"d_ff must be an integer > 0\")\n",
    "        if not isinstance(self.dropout, float) or not (0 <= self.dropout <= 1):\n",
    "            raise ValueError(\"dropout must be a float between 0 and 1\")\n",
    "        if self.activation_function not in ('gelu', 'relu', 'tanh', 'sigmoid'):\n",
    "            raise ValueError(\"activation_function must be one of: 'gelu', 'relu', 'tanh', 'sigmoid'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.load_config(config)\n",
    "    \n",
    "    def load_config(self, config: GPT2Config):\n",
    "        self.config = config\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_k = self.d_model // self.n_head\n",
    "\n",
    "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
    "        self.w_k = nn.Linear(self.d_model, self.d_model)\n",
    "        self.w_v = nn.Linear(self.d_model, self.d_model)\n",
    "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        Q = self.w_q(x)\n",
    "        K = self.w_k(x)\n",
    "        V = self.w_v(x)\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.n_head, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_head, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_head, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        out = self.w_o(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.load_config(config)\n",
    "        \n",
    "    def load_config(self, config: GPT2Config):\n",
    "        self.config = config\n",
    "        self.d_model = config.d_model\n",
    "        self.d_ff = config.d_ff\n",
    "        self.activation_function = config.activation_function\n",
    "\n",
    "        self.fc1 = nn.Linear(self.d_model, self.d_ff) # type: ignore\n",
    "        self.fc2 = nn.Linear(self.d_ff, self.d_model) # type: ignore\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        if self.activation_function == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "        elif self.activation_function == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif self.activation_function == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif self.activation_function == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.load_config(config)\n",
    "        \n",
    "    def load_config(self, config: GPT2Config):\n",
    "        self.config = config\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.attention(x, mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_out)\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.load_config(config)\n",
    "        \n",
    "    def load_config(self, config: GPT2Config):\n",
    "        self.config = config\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.position_embedding = nn.Embedding(config.block_size, config.d_model)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layer)])\n",
    "        self.norm = nn.LayerNorm(config.d_model)\n",
    "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.head.weight = self.token_embedding.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        pos_embeds = self.position_embedding(positions)\n",
    "        x = token_embeds + pos_embeds\n",
    "\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device), diagonal=1)\n",
    "        mask = mask == 1\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                           shift_labels.view(-1))\n",
    "\n",
    "        return {'logits': logits, 'loss': loss}\n",
    "    \n",
    "    def generate(self, input_ids, max_length=50, temperature=1.0, top_k=50):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                outputs = self(input_ids)\n",
    "                next_token_logits = outputs['logits'][:, -1, :] / temperature\n",
    "\n",
    "                if top_k > 0:\n",
    "                    top_k = min(top_k, next_token_logits.size(-1))\n",
    "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
    "\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "                if next_token.item() == 50256:\n",
    "                    break\n",
    "        \n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, block_size, stoi):\n",
    "        self.block_size = block_size\n",
    "        self.stoi = stoi\n",
    "        self.data = [self.stoi[c] for c in text if c in self.stoi]\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.block_size], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx+1:idx+self.block_size+1], dtype=torch.long)\n",
    "        return x, y\n",
    "def train_gpt2_tiny_shakespeare(config: GPT2Config, device='cuda', epochs=1, batch_size=32, lr=3e-4, fp16=True):\n",
    "    dataset = load_dataset('tiny_shakespeare')['train'] #type: ignore\n",
    "    text = dataset[0]['text']\n",
    "    vocab = sorted(list(set(text)))\n",
    "    stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "    itos = {i: ch for ch, i in stoi.items()}\n",
    "    config.vocab_size = len(vocab)\n",
    "\n",
    "    ds = CharDataset(text, config.block_size, stoi)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    model = GPT2Model(config).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training!\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=fp16)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dl, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda', enabled=fp16):\n",
    "                out = model(x, y)\n",
    "                loss = out['loss'] if not isinstance(model, torch.nn.DataParallel) else out['loss']\n",
    "            if loss is not None:\n",
    "                if loss.dim() > 0:\n",
    "                    loss = loss.mean()\n",
    "                if not torch.isnan(loss):\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\" if loss is not None else 'None'})\n",
    "\n",
    "    return model, stoi, itos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import GPT2Config\n",
    "from train import train_gpt2_tiny_shakespeare\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    config = GPT2Config(block_size=128, n_layer=4, n_head=4, d_model=128, dropout=0.1)\n",
    "    print(\"Training GPT-2 on tiny Shakespeare (FP16)...\")\n",
    "    model, stoi, itos = train_gpt2_tiny_shakespeare(config, device=device, epochs=1, batch_size=32, lr=3e-4, fp16=True)\n",
    "\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        print(\"Model is wrapped in DataParallel.\")\n",
    "    elif torch.cuda.device_count() > 1:\n",
    "        print(f\"Wrapping model in DataParallel for inference on {torch.cuda.device_count()} GPUs!\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    torch.save(model.state_dict(), \"gpt2_tiny_shakespeare.pt\")\n",
    "    print(\"Model saved to gpt2_tiny_shakespeare.pt\")\n",
    "\n",
    "    prompt = \"ROMEO: \"\n",
    "    input_ids = torch.tensor([[stoi[c] for c in prompt]], dtype=torch.long).to(device)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        out_ids = model.module.generate(input_ids, max_length=100) if isinstance(model, torch.nn.DataParallel) else model.generate(input_ids, max_length=100)\n",
    "    out_text = ''.join([itos[i] for i in out_ids[0].tolist()])\n",
    "    print(\"\\nSample generated text:\\n\")\n",
    "    print(out_text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
